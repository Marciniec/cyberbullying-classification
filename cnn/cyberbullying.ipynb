{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk import PorterStemmer, RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load corpus with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/training_set_clean_only_text.txt', 'r', encoding=\"utf8\") as text_file:\n",
    "    text_lines = text_file.readlines()\n",
    "with open('dataset/training_set_clean_only_tags.txt', 'r') as tags_file:\n",
    "    tags = tags_file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load word embedings found in http://dsmodels.nlp.ipipan.waw.pl/ \n",
    "continuous Skip-gram (SG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_text = KeyedVectors.load_word2vec_format(datapath(r\"C:\\Users\\Anna Marciniec\\dataset\\nkjp-forms-all-300-skipg-hs.txt.gz\"), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    cleaned =  ' '.join(re.sub(\"(@[A-Za-z0-9]_+)|([^0-9A-Za-z \\t]) |(\\w+:\\/\\/\\S+) |@anonymized_account \", \" \", sentence).split())\n",
    "    cleaned = cleaned.translate(str.maketrans('', '', string.punctuation))\n",
    "    return cleaned.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    cleaned_tweet = clean_sentence(tweet)\n",
    "    new_tweet_list = []\n",
    "    tokenized =  regexp_tokenizer.tokenize(cleaned_tweet.lower())\n",
    "    for word in tokenized:\n",
    "        if word in wv_from_text.vocab:\n",
    "            new_tweet_list.append(word)\n",
    "            continue\n",
    "        if word.capitalize() in wv_from_text.vocab:\n",
    "            new_tweet_list.append(word.capitalize())\n",
    "            continue\n",
    "        removed = word[:-2]\n",
    "        if removed in wv_from_text.vocab:\n",
    "            new_tweet_list.append(removed)\n",
    "            continue\n",
    "    return ' '.join(new_tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tweet in text_lines:\n",
    "    new_tweet = clean_tweet(tweet)\n",
    "    cleaned_tweets.append(new_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset for test and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cleaned_tweets, tags, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = list(map(int, y_train))\n",
    "y_test = list(map(int, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(cleaned_tweets)\n",
    "word_index = token.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = set(itertools.chain(*list(map(lambda x :x.split(' ') ,cleaned_tweets))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(lexicon) + 1, 300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    if word in wv_from_text.vocab:\n",
    "        embedding_vector = wv_from_text.get_vector(word)\n",
    "    else:\n",
    "        embedding_vector = wv_from_text.get_vector(word.capitalize())\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, metrics\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7530 samples\n",
      "Epoch 1/11\n",
      "7530/7530 [==============================] - 3s 450us/sample - loss: 0.2745\n",
      "Epoch 2/11\n",
      "7530/7530 [==============================] - 3s 375us/sample - loss: 0.2023s - loss: 0.227 - ETA: 2s - l - ETA: 1s - loss:  - ETA\n",
      "Epoch 3/11\n",
      "7530/7530 [==============================] - 3s 374us/sample - loss: 0.1680\n",
      "Epoch 4/11\n",
      "7530/7530 [==============================] - 3s 375us/sample - loss: 0.1397s - loss: 0\n",
      "Epoch 5/11\n",
      "7530/7530 [==============================] - 3s 374us/sample - loss: 0.1122s - loss\n",
      "Epoch 6/11\n",
      "7530/7530 [==============================] - 3s 374us/sample - loss: 0.0752- ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.07\n",
      "Epoch 7/11\n",
      "7530/7530 [==============================] - 3s 376us/sample - loss: 0.0609\n",
      "Epoch 8/11\n",
      "7530/7530 [==============================] - 3s 381us/sample - loss: 0.0528\n",
      "Epoch 9/11\n",
      "7530/7530 [==============================] - 3s 374us/sample - loss: 0.0422s - loss:\n",
      "Epoch 10/11\n",
      "7530/7530 [==============================] - 3s 378us/sample - loss: 0.0308s -  - ETA: 0s - loss: 0.0\n",
      "Epoch 11/11\n",
      "7530/7530 [==============================] - 3s 379us/sample - loss: 0.0337s - ETA:  - ETA: 0s - loss:\n",
      "Accuracy 0.9203504579848666\n"
     ]
    }
   ],
   "source": [
    "def create_cnn():\n",
    "    with tf.device('/GPU:0'):\n",
    "        # Add an Input Layer\n",
    "        input_layer = layers.Input((70, ))\n",
    "\n",
    "        # Add the word embedding Layer\n",
    "        embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "        embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "        # Add the convolutional Layer\n",
    "        conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "        # Add the pooling Layer\n",
    "        pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "        # Add the output Layers\n",
    "        output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "        output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "        output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "        # Compile the model\n",
    "        model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "        model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn()\n",
    "accuracy = train_model(classifier, train_seq_x, np.array(y_train), valid_seq_x, is_neural_net=True)\n",
    "print (\"Accuracy\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    with tf.device('/GPU:0'):\n",
    "        # fit the training dataset on the classifier\n",
    "        classifier.fit(feature_vector_train, label, epochs = 11)\n",
    "\n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(feature_vector_valid)\n",
    "\n",
    "        if is_neural_net:\n",
    "            predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    return metrics.accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence:str):\n",
    "    cleaned_tweet = clean_tweet(sentence)\n",
    "    seq = sequence.pad_sequences(token.texts_to_sequences([cleaned_tweet]), maxlen=70)\n",
    "    prediction = loaded_classifier.predict(seq)\n",
    "    text = ''\n",
    "    if prediction[0] >= 0.5:\n",
    "        text = 'cyberbulling'\n",
    "    else:\n",
    "        text = 'not cyberbulling'\n",
    "    print(f'with classification of {prediction[0][0]} sentence \"{sentence}\" is classified as {text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://twitter.com/RobertBiedron/status/1216358527222108161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with classification of 0.00016966761904768646 sentence \"do końca świata i o jeden dzień dłużej\" is classified as not cyberbulling\n"
     ]
    }
   ],
   "source": [
    "predict('do końca świata i o jeden dzień dłużej')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source : https://twitter.com/KingaBezKorony/status/1216447484169543682, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with classification of 0.777967631816864 sentence \"Ty małostkowy, podły, bezduszny, wstrętny, posiorski katolu!!! siema! \" is classified as cyberbulling\n"
     ]
    }
   ],
   "source": [
    "predict('Ty małostkowy, podły, bezduszny, wstrętny, posiorski katolu!!! siema! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('cyberbulling_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_classifier = keras.models.load_model('cyberbulling_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source https://twitter.com/jurema4444/status/1217071156760629249"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with classification of 0.6486673951148987 sentence \"Donosiciele, zdrajcy najwieksze łajzy\" is classified as cyberbulling\n"
     ]
    }
   ],
   "source": [
    "predict('Donosiciele, zdrajcy najwieksze łajzy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
