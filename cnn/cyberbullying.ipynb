{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk import PorterStemmer, RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/training_set_clean_only_text.txt', 'r', encoding=\"utf8\") as text_file:\n",
    "    text_lines = text_file.readlines()\n",
    "with open('dataset/training_set_clean_only_tags.txt', 'r') as tags_file:\n",
    "    tags = tags_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_text = KeyedVectors.load_word2vec_format(datapath(r\"C:\\Users\\Anna Marciniec\\dataset\\nkjp-forms-all-300-skipg-hs.txt.gz\"), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    cleaned =  ' '.join(re.sub(\"(@[A-Za-z0-9]_+)|([^0-9A-Za-z \\t]) |(\\w+:\\/\\/\\S+) |@anonymized_account \", \" \", sentence).split())\n",
    "    cleaned = cleaned.translate(str.maketrans('', '', string.punctuation))\n",
    "    return cleaned.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    cleaned_tweet = clean_sentence(tweet)\n",
    "    new_tweet_list = []\n",
    "    tokenized =  regexp_tokenizer.tokenize(cleaned_tweet.lower())\n",
    "    for word in tokenized:\n",
    "        if word in wv_from_text.vocab:\n",
    "            new_tweet_list.append(word)\n",
    "            continue\n",
    "        if word.capitalize() in wv_from_text.vocab:\n",
    "            new_tweet_list.append(word.capitalize())\n",
    "            continue\n",
    "        removed = word[:-2]\n",
    "        if removed in wv_from_text.vocab:\n",
    "            new_tweet_list.append(removed)\n",
    "            continue\n",
    "    return ' '.join(new_tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tweet in text_lines:\n",
    "    new_tweet = clean_tweet(tweet)\n",
    "    cleaned_tweets.append(new_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cleaned_tweets, tags, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = list(map(int, y_train))\n",
    "y_test = list(map(int, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(cleaned_tweets)\n",
    "word_index = token.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = set(itertools.chain(*list(map(lambda x :x.split(' ') ,cleaned_tweets))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(lexicon) + 1, 300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    if word in wv_from_text.vocab:\n",
    "        embedding_vector = wv_from_text.get_vector(word)\n",
    "    else:\n",
    "        embedding_vector = wv_from_text.get_vector(word.capitalize())\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.094872  ,  0.135253  ,  0.013441  , ..., -0.048842  ,\n",
       "         0.000602  , -0.105519  ],\n",
       "       [-0.06228   , -0.001151  , -0.024933  , ...,  0.052002  ,\n",
       "         0.066079  ,  0.089732  ],\n",
       "       ...,\n",
       "       [-0.011147  ,  0.022036  ,  0.22927   , ..., -0.30707601,\n",
       "         0.082493  , -0.092688  ],\n",
       "       [ 0.263311  ,  0.092519  , -0.120999  , ...,  0.32495001,\n",
       "        -0.145742  , -0.33339599],\n",
       "       [-0.030147  , -0.15119299, -0.20597699, ...,  0.09103   ,\n",
       "        -0.238083  , -0.052658  ]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers, models, optimizers, metrics\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7530/7530 [==============================] - 4s 592us/step - loss: 0.2847\n",
      "Epoch 2/10\n",
      "7530/7530 [==============================] - 4s 520us/step - loss: 0.2050\n",
      "Epoch 3/10\n",
      "7530/7530 [==============================] - 4s 521us/step - loss: 0.1811\n",
      "Epoch 4/10\n",
      "7530/7530 [==============================] - 4s 521us/step - loss: 0.1538\n",
      "Epoch 5/10\n",
      "7530/7530 [==============================] - 4s 519us/step - loss: 0.1275\n",
      "Epoch 6/10\n",
      "7530/7530 [==============================] - ETA: 0s - loss: 0.096 - 4s 519us/step - loss: 0.0966\n",
      "Epoch 7/10\n",
      "7530/7530 [==============================] - 4s 521us/step - loss: 0.0754\n",
      "Epoch 8/10\n",
      "7530/7530 [==============================] - 4s 519us/step - loss: 0.0599\n",
      "Epoch 9/10\n",
      "7530/7530 [==============================] - 4s 521us/step - loss: 0.0466\n",
      "Epoch 10/10\n",
      "7530/7530 [==============================] - 4s 524us/step - loss: 0.0346\n",
      "[[0.8445803]]\n",
      "CNN, Word Embeddings 0.9199522102747909\n"
     ]
    }
   ],
   "source": [
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn()\n",
    "accuracy = train_model(classifier, train_seq_x, y_train, valid_seq_x, is_neural_net=True)\n",
    "print (\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label, epochs = 10)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    print(classifier.predict(ex_1))\n",
    "    return metrics.accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "example  = ['Do końca świata i o jeden dzień dłużej!', 'ty małoskowy, podły bezduszny wstrętny pisiorski katolu siema']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources : https://twitter.com/KingaBezKorony/status/1216447484169543682, https://twitter.com/RobertBiedron/status/1216358527222108161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_1 = sequence.pad_sequences(token.texts_to_sequences(example), maxlen=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.8819788e-05],\n",
       "       [7.2009170e-01]], dtype=float32)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(ex_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
